algo:
  name: original
  self_play: true
envparameters:
  num_actions: 4
  state_shape: None
hyperparameters:
  anticipatory_param: 0.5
  evaluate_with: average_policy
rl:
  batch_size: 256
  discount_factor: 0.99
  epsilon_decay_steps: 100000
  epsilon_end: 0.1
  epsilon_start: 1
  learning_rate: 5.0e-05
  mlp_layers:
  - 512
  - 1024
  - 2048
  - 1024
  - 512
  replay_memory_init_size: 1000
  replay_memory_size: 60000
  train_every: 64
  update_target_estimator_every: 1000
sl:
  batch_size: 256
  hidden_layers_sizes:
  - 512
  - 1024
  - 2048
  - 1024
  - 512
  learning_rate: 0.0001
  min_buffer_size_to_learn: 1000
  reservoir_buffer_capacity: 30000
  train_every: 64
trainparameters:
  eval_every: 1000
  num_episodes: 50000
  num_eval_games: 100
